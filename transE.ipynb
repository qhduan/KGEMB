{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'WN18/train.txt'\n",
    "test_file = 'WN18/train.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    data = []\n",
    "    with open(path) as fp:\n",
    "        for line in fp:\n",
    "            line = line.strip()\n",
    "            r = line.split('\\t')\n",
    "            if len(r) == 3:\n",
    "                data.append(r)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_data(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = load_data(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words实际上是实体+关系的字符串\n",
    "words = Counter()\n",
    "for x in train:\n",
    "    words.update(x)\n",
    "for x in test:\n",
    "    words.update(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40961\n"
     ]
    }
   ],
   "source": [
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 大约是words数量的10倍\n",
    "n_token = 50 * 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(tf.keras.models.Model):\n",
    "    \n",
    "    def __init__(self, num_buckets = 10 * 10000):\n",
    "        super(Tokenizer, self).__init__()\n",
    "        self.num_buckets = num_buckets\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = tf.strings.to_hash_bucket_fast(inputs, self.num_buckets)\n",
    "        x = tf.cast(x, dtype=tf.float32)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_real = tf.keras.Input(shape=(None,), dtype='string')\n",
    "input_fake = tf.keras.Input(shape=(None,), dtype='string')\n",
    "\n",
    "tok = Tokenizer(n_token)\n",
    "emb = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(None,), dtype=tf.float32),\n",
    "    tf.keras.layers.Embedding(n_token, 64)\n",
    "])\n",
    "\n",
    "real = input_real\n",
    "fake = input_fake\n",
    "\n",
    "real = tok(real)\n",
    "fake = tok(fake)\n",
    "\n",
    "\n",
    "head = real[:, 0]\n",
    "rel = real[:, 1]\n",
    "tail = real[:, 2]\n",
    "head = emb(head)\n",
    "rel = emb(rel)\n",
    "tail = emb(tail)\n",
    "\n",
    "distance = head + rel - tail\n",
    "dis_real = tf.linalg.normalize(distance, axis=1)[1]\n",
    "\n",
    "head = fake[:, 0]\n",
    "rel = fake[:, 1]\n",
    "tail = fake[:, 2]\n",
    "head = emb(head)\n",
    "rel = emb(rel)\n",
    "tail = emb(tail)\n",
    "\n",
    "distance = head + rel - tail\n",
    "dis_fake = tf.linalg.normalize(distance, axis=1)[1]\n",
    "\n",
    "x = dis_real - dis_fake\n",
    "\n",
    "model = tf.keras.models.Model(\n",
    "    inputs=[input_real, input_fake],\n",
    "    outputs=x\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    # 自定义loss函数\n",
    "    # loss(x, y) = max(0, -y * (x1 - x2) + margin)\n",
    "    # y = -1\n",
    "    # margin = 1.0\n",
    "    loss=lambda true, pred: tf.math.maximum(0.0, pred + true)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generate(data, batch_size=32):\n",
    "    \"\"\"数据生成，每个迭代返回pos和neg两个sample集合，还有margin\"\"\"\n",
    "    entities = {}\n",
    "    for x in data:\n",
    "        if x[0] not in entities:\n",
    "            entities[x[0]] = {}\n",
    "        if x[2] not in entities:\n",
    "            entities[x[2]] = {}\n",
    "        if x[1] not in entities[x[0]]:\n",
    "            entities[x[0]][x[1]] = []\n",
    "        if x[1] not in entities[x[2]]:\n",
    "            entities[x[2]][x[1]] = []\n",
    "        entities[x[0]][x[1]].append(x[2])\n",
    "        entities[x[2]][x[1]].append(x[0])\n",
    "    words = list(entities.keys())\n",
    "    n_batch = math.ceil(len(data) / batch_size)\n",
    "    def _get_random(x0, r, x1):\n",
    "        while True:\n",
    "            neg = random.choice(words)\n",
    "            if neg != x0 and neg != x1:\n",
    "                if neg not in entities[x0][r]:\n",
    "                    if neg not in entities[x1][r]:\n",
    "                        return neg\n",
    "    for i in range(n_batch):\n",
    "        batch = data[i * batch_size: (i + 1) * batch_size]\n",
    "        neg = [\n",
    "            [x0, r, _get_random(x0, r, x1)]\n",
    "            for x0, r, x1 in batch\n",
    "        ]\n",
    "        y = np.array([[1.]] * len(batch))  # margin\n",
    "        yield [np.array(batch), np.array(neg)], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 0.9707: 100%|██████████| 139/139 [01:03<00:00,  2.17it/s]\n",
      "epoch: 1 loss: 0.8725: 100%|██████████| 139/139 [01:03<00:00,  2.18it/s]\n",
      "epoch: 2 loss: 0.7729: 100%|██████████| 139/139 [01:03<00:00,  2.19it/s]\n",
      "epoch: 3 loss: 0.6872: 100%|██████████| 139/139 [01:04<00:00,  2.16it/s]\n",
      "epoch: 4 loss: 0.6057: 100%|██████████| 139/139 [01:04<00:00,  2.16it/s]\n",
      "epoch: 5 loss: 0.5311: 100%|██████████| 139/139 [01:04<00:00,  2.15it/s]\n",
      "epoch: 6 loss: 0.4607: 100%|██████████| 139/139 [01:05<00:00,  2.14it/s]\n",
      "epoch: 7 loss: 0.3963: 100%|██████████| 139/139 [01:04<00:00,  2.15it/s]\n",
      "epoch: 8 loss: 0.3413: 100%|██████████| 139/139 [01:04<00:00,  2.14it/s]\n",
      "epoch: 9 loss: 0.2956: 100%|██████████| 139/139 [01:04<00:00,  2.16it/s]\n",
      "epoch: 10 loss: 0.2563: 100%|██████████| 139/139 [01:04<00:00,  2.15it/s]\n",
      "epoch: 11 loss: 0.2239: 100%|██████████| 139/139 [01:04<00:00,  2.14it/s]\n",
      "epoch: 12 loss: 0.1976: 100%|██████████| 139/139 [01:04<00:00,  2.15it/s]\n",
      "epoch: 13 loss: 0.1754: 100%|██████████| 139/139 [01:04<00:00,  2.16it/s]\n",
      "epoch: 14 loss: 0.1583: 100%|██████████| 139/139 [01:04<00:00,  2.15it/s]\n",
      "epoch: 15 loss: 0.1430: 100%|██████████| 139/139 [01:04<00:00,  2.15it/s]\n",
      "epoch: 16 loss: 0.1282: 100%|██████████| 139/139 [01:05<00:00,  2.12it/s]\n",
      "epoch: 17 loss: 0.1165: 100%|██████████| 139/139 [01:04<00:00,  2.14it/s]\n",
      "epoch: 18 loss: 0.1054: 100%|██████████| 139/139 [01:04<00:00,  2.14it/s]\n",
      "epoch: 19 loss: 0.0964: 100%|██████████| 139/139 [01:04<00:00,  2.14it/s]\n",
      "epoch: 20 loss: 0.0886: 100%|██████████| 139/139 [01:04<00:00,  2.16it/s]\n",
      "epoch: 21 loss: 0.0807: 100%|██████████| 139/139 [01:04<00:00,  2.17it/s]\n",
      "epoch: 22 loss: 0.0742: 100%|██████████| 139/139 [01:04<00:00,  2.17it/s]\n",
      "epoch: 23 loss: 0.0682: 100%|██████████| 139/139 [01:03<00:00,  2.19it/s]\n",
      "epoch: 24 loss: 0.0627: 100%|██████████| 139/139 [01:04<00:00,  2.16it/s]\n",
      "epoch: 25 loss: 0.0580: 100%|██████████| 139/139 [01:05<00:00,  2.14it/s]\n",
      "epoch: 26 loss: 0.0537: 100%|██████████| 139/139 [01:04<00:00,  2.16it/s]\n",
      "epoch: 27 loss: 0.0496: 100%|██████████| 139/139 [01:05<00:00,  2.13it/s]\n",
      "epoch: 28 loss: 0.0460: 100%|██████████| 139/139 [01:04<00:00,  2.17it/s]\n",
      "epoch: 29 loss: 0.0427: 100%|██████████| 139/139 [01:05<00:00,  2.13it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "n_batch = math.ceil(len(train) / batch_size)\n",
    "for epoch in range(30):\n",
    "    pbar = tqdm(data_generate(train, batch_size), total=n_batch)\n",
    "    losses = []\n",
    "    for [pos, neg], y in pbar:\n",
    "        loss = model.train_on_batch([pos, neg], y)\n",
    "        losses.append(loss)\n",
    "        pbar.set_description(f'epoch: {epoch} loss: {np.mean(losses):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tok/assets\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "tok.save('/tmp/tok')\n",
    "emb.save('/tmp/emb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = sorted(words.keys())\n",
    "rel = [x for x in entities if x.startswith('_')]\n",
    "entities = [x for x in entities if not x.startswith('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = tf.keras.models.load_model('/tmp/tok')\n",
    "emb = tf.keras.models.load_model('/tmp/emb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 40943\n"
     ]
    }
   ],
   "source": [
    "print(len(rel), len(entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_vecs = emb(tok(tf.constant([entities]))).numpy()[0]\n",
    "rel_vecs = emb(tok(tf.constant([rel]))).numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40943, 64) (18, 64)\n"
     ]
    }
   ],
   "source": [
    "print(ent_vecs.shape, rel_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "good: 7136, bad: 1756, total: 8892, hit@10: 0.8025:   6%|▋         | 8891/141442 [20:27<5:08:28,  7.16it/s]"
     ]
    }
   ],
   "source": [
    "# 计算 Hit@10\n",
    "ret = []\n",
    "good, bad = 0, 0\n",
    "pbar = tqdm(test)\n",
    "for a, b, c in pbar:\n",
    "    target = emb(tok(tf.constant([[a]]))) \\\n",
    "            + emb(tok(tf.constant([[b]])))\n",
    "    diss = tf.linalg.normalize(target - ent_vecs, axis=2)[1].numpy().flatten()\n",
    "    if c in [entities[i] for i in np.argsort(diss)[:10]]:\n",
    "        good += 1\n",
    "    else:\n",
    "        bad += 1\n",
    "    pbar.set_description(\n",
    "        f'good: {good}, bad: {bad}, total: {good + bad}, hit@10: {good / (good + bad):.4f}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/thunlp/KB2E\n",
    "# Hit@10(raw) transE = 75.4 or 78.9\n",
    "print(good, bad, len(test), good / len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
